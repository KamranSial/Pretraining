{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Simple tester for the vgg19_trainable\n",
    "\"\"\"\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "import vgg_fcn_combined as vgg16\n",
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "from IPython.display import clear_output\n",
    "import sys\n",
    "import csv\n",
    "\n",
    "PRETRAINED_MODEL_PATH= \"/mnt/data3/gap/unrel/ckpt_dir/\"\n",
    "N_EPOCHS = 300\n",
    "INIT_LEARNING_RATE = 0.01\n",
    "WEIGHT_DECAY_RATE = 0.0005\n",
    "MOMENTUM = 0.9\n",
    "IMAGE_HEIGHT  = 224    #960\n",
    "IMAGE_WIDTH   = 224    #720\n",
    "NUM_CHANNELS  = 3\n",
    "BATCH_SIZE = 90\n",
    "N_CLASSES = 1000\n",
    "DROPOUT = 0.50\n",
    "ckpt_dir = \"/mnt/data3/gap/unrel/ckpt_dir/\"\n",
    "#LOGS_PATH = '/home/sik4hi/tensorflow_logs'\n",
    "WEIGHT_PATH = '.npy'\n",
    "TRAINSET_PATH = '/mnt/data1/unrelated_dataset2/unrelated_csv2/unrelated_image_labels.csv'\n",
    "VALSET_PATH ='/mnt/data1/unrelated_dataset2/unrelated_csv2/val_unrelated_image_labels.csv'\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "#=======================================================================================================\n",
    "# Reading Training data from CSV FILE\n",
    "#=======================================================================================================\n",
    "train_csv_path=open(TRAINSET_PATH,\"r\")\n",
    "train_filepaths=[]\n",
    "train_labels=[]\n",
    "for line in train_csv_path:\n",
    "    filepath, label= line.split(\",\")\n",
    "    label= int(label)\n",
    "    train_filepaths.append(filepath)\n",
    "    train_labels.append(label)\n",
    "    \n",
    "val_csv_file=open(VALSET_PATH,\"r\")\n",
    "val_filepaths=[]\n",
    "val_labels=[]\n",
    "for line in val_csv_file:\n",
    "    filepath, label= line.split(\",\")\n",
    "    label= int(label)\n",
    "    val_filepaths.append(filepath)\n",
    "    val_labels.append(label)\n",
    "    \n",
    "with tf.device('/cpu:0'):\n",
    "    def _compute_longer_edge(height, width, new_shorter_edge):\n",
    "        return tf.cast(width*new_shorter_edge/height, tf.int32)\n",
    "\n",
    "    \n",
    "    train_image_path , train_label = tf.train.slice_input_producer([train_filepaths,train_labels]\n",
    "                                           ,capacity = 142000, shuffle=True)\n",
    "    \n",
    "    \n",
    "    train_image_content = tf.read_file(train_image_path)\n",
    "    train_image = tf.image.decode_jpeg(train_image_content, channels=3)\n",
    "    \n",
    "    shape = tf.shape(train_image)\n",
    "    height = shape[0]\n",
    "    width = shape[1]\n",
    "    new_shorter_edge = tf.constant(256, dtype=tf.int32)\n",
    "\n",
    "    height_smaller_than_width = tf.less_equal(height, width)\n",
    "    new_height_and_width = tf.cond(\n",
    "        height_smaller_than_width,\n",
    "        lambda: (new_shorter_edge, _compute_longer_edge(height, width, new_shorter_edge)),\n",
    "        lambda: (_compute_longer_edge(width, height, new_shorter_edge), new_shorter_edge)\n",
    "    )\n",
    "    size = tf.cast([new_height_and_width[0], new_height_and_width[1]], tf.int32)\n",
    "    train_image = tf.image.resize_images(train_image, size)\n",
    "    size = tf.cast([IMAGE_HEIGHT, IMAGE_WIDTH, 3], tf.int32)\n",
    "    train_image = tf.random_crop(train_image, size)\n",
    "    train_image = tf.image.random_flip_left_right(train_image)\n",
    "    train_image = tf.cast(train_image, tf.float32)/255. # necessary for mapping rgb channels from 0-255 to 0-1 float.\n",
    "    train_label = tf.cast(train_label, tf.int64) # unnecessary\n",
    "    train_image_batch, train_label_batch = tf.train.batch([train_image, train_label], batch_size=BATCH_SIZE,\n",
    "                                                                  capacity = 1000 + 3*BATCH_SIZE, \n",
    "                                                                  num_threads=7)\n",
    "    \n",
    "    \n",
    "    val_image_path , val_label = tf.train.slice_input_producer([val_filepaths,val_labels]\n",
    "                                           ,capacity = 10000)\n",
    "    \n",
    "    \n",
    "    val_image_content = tf.read_file(val_image_path)\n",
    "    val_image = tf.image.decode_jpeg(val_image_content, channels=3)\n",
    "    \n",
    "    shape = tf.shape(val_image)\n",
    "    height = shape[0]\n",
    "    width = shape[1]\n",
    "    new_shorter_edge = tf.constant(256, dtype=tf.int32)\n",
    "\n",
    "    height_smaller_than_width = tf.less_equal(height, width)\n",
    "    new_height_and_width = tf.cond(\n",
    "        height_smaller_than_width,\n",
    "        lambda: (new_shorter_edge, _compute_longer_edge(height, width, new_shorter_edge)),\n",
    "        lambda: (_compute_longer_edge(width, height, new_shorter_edge), new_shorter_edge)\n",
    "    )\n",
    "    size = tf.cast([new_height_and_width[0], new_height_and_width[1]], tf.int32)\n",
    "    val_image = tf.image.resize_images(val_image, size)\n",
    "    \n",
    "    size = tf.cast([IMAGE_HEIGHT, IMAGE_WIDTH, 3], tf.int32)\n",
    "    val_image = tf.random_crop(val_image, size)\n",
    "    \n",
    "    #val_image = tf.image.random_flip_left_right(val_image)\n",
    "    val_image = tf.cast(val_image, tf.float32)/255. # necessary for mapping rgb channels from 0-255 to 0-1 float.\n",
    "    val_label = tf.cast(val_label, tf.int64) # unnecessary\n",
    "    val_image_batch, val_label_batch = tf.train.batch([val_image, val_label], batch_size=BATCH_SIZE,\n",
    "                                                                  capacity = 1000 + 3*BATCH_SIZE, \n",
    "                                                                  num_threads=7)\n",
    "    \n",
    "with tf.device('/gpu:0'):\n",
    "   \n",
    "    \n",
    "    \n",
    "    #images_tf = tf.placeholder(tf.float32, [None, 224, 224, 3])\n",
    "    #labels_tf = tf.placeholder(tf.int64)\n",
    "    train_mode = tf.placeholder(tf.bool)\n",
    "\n",
    "    vgg = vgg16.Vgg16()\n",
    "    \n",
    "    if train_mode is not None:\n",
    "        images_tf=tf.cond(train_mode, lambda:train_image_batch, lambda:val_image_batch)\n",
    "    if train_mode is not None:\n",
    "        labels_tf=tf.cond(train_mode, lambda:train_label_batch, lambda:val_label_batch)\n",
    "                          \n",
    "    \n",
    "    vgg.build(images_tf, train_mode=train_mode)\n",
    "\n",
    "    #==============================================================================================================\n",
    "    # Defining Loss, could be changed from cross entropy depending on needs. The current configuration works well on\n",
    "    # multiclass (not hot-encoded vectors) prediction like ImageNET.\n",
    "    #==============================================================================================================\n",
    "    with tf.name_scope('Loss'):\n",
    "        loss_tf = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(vgg.fc8, labels_tf), name='loss_tf')\n",
    "        l2_loss=tf.reduce_sum(tf.get_collection(\"losses\"))\n",
    "        \n",
    "        loss_tf2 =loss_tf + l2_loss\n",
    "\n",
    "    # ==============================================================================================================\n",
    "    # Optimizer, again it can be changed to any function provided by Tensorflow. You can simply use commented out line\n",
    "    # instead of explicitly computing gradients, if you are not interested in creating summaries of gradients.\n",
    "    # ==============================================================================================================\n",
    "    #train_op = tf.train.MomentumOptimizer(learning_rate, MOMENTUM).minimize(loss_tf2)\n",
    "    train_op = tf.train.AdamOptimizer(learning_rate=0.01,epsilon=0.1).minimize(loss_tf2)\n",
    "    #optimizer = tf.train.MomentumOptimizer(learning_rate, MOMENTUM)\n",
    "    #grads_and_vars = optimizer.compute_gradients(loss_tf)\n",
    "    #grads_and_vars = map(\n",
    "     #   lambda gv: (gv[0], gv[1]) if ('conv6' in gv[1].name or 'GAP' in gv[1].name) else (gv[0] * 0.1, gv[1]),\n",
    "      #  grads_and_vars)\n",
    "    #grads_and_vars = [(tf.clip_by_value(gv[0], -5., 5.), gv[1]) for gv in grads_and_vars]\n",
    "    #train_op = optimizer.apply_gradients(grads_and_vars)\n",
    "\n",
    "\n",
    "    # ===================================================================================================================\n",
    "    # Accuracy for the current batch\n",
    "    # ===================================================================================================================\n",
    "    correct_pred = tf.equal(tf.argmax(vgg.fc8, 1), labels_tf)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "with tf.device('/cpu:0'):\n",
    "    accuracy_top5= tf.reduce_mean(tf.cast(tf.nn.in_top_k(vgg.fc8,labels_tf,5), tf.float32))\n",
    "\n",
    "with tf.device('/cpu:0'):    \n",
    "    if not os.path.exists(ckpt_dir):\n",
    "        os.makedirs(ckpt_dir)\n",
    "    saver = tf.train.Saver(max_to_keep=150)\n",
    "    \n",
    "with tf.device('/gpu:0'):\n",
    "    sess = tf.Session()\n",
    "    init_op = tf.group(tf.initialize_all_variables(),\n",
    "                       tf.initialize_local_variables())\n",
    "    sess.run(init_op)\n",
    "    if PRETRAINED_MODEL_PATH:\n",
    "        print \"using Pretrained model\"\n",
    "        ckpt = tf.train.get_checkpoint_state(PRETRAINED_MODEL_PATH)\n",
    "        print(ckpt)\n",
    "        saver.restore(sess,ckpt.model_checkpoint_path)\n",
    "        \n",
    "    coord=tf.train.Coordinator()\n",
    "    threads = tf.train.start_queue_runners(sess=sess, coord=coord)\n",
    "    \n",
    "    print(\"finished\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "epoch=54"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "train_loss_list, train_accuracy_list, plot_train_loss, plot_train_accuracy,l2_loss_list, plot_l2_loss =  [], [], [], [], [], []\n",
    "train_top5_list, plot_train_top5, val_top5_list, plot_val_top5 = [], [], [], []\n",
    "val_loss_list , val_accuracy_list ,plot_val_loss, plot_val_accuracy = [], [], [], []\n",
    "\n",
    "try:   \n",
    "    while not coord.should_stop() and epoch< N_EPOCHS:\n",
    "        epoch+=1\n",
    "        epoch_start_time = time.time()\n",
    "        print((142000 / BATCH_SIZE))\n",
    "        for i in xrange((142000/ BATCH_SIZE)):\n",
    "            _, train_loss,l2_train_loss, train_accuracy ,train_top5= sess.run(\n",
    "               [train_op, loss_tf, l2_loss, accuracy, accuracy_top5],feed_dict={train_mode: True})\n",
    "\n",
    "            train_loss_list.append(train_loss)\n",
    "            train_accuracy_list.append(train_accuracy)\n",
    "            l2_loss_list.append(l2_train_loss)\n",
    "            train_top5_list.append(train_top5)\n",
    "            sys.stdout.write('\\r\\r' + \"Iteration:\" + str(i)+ \"  Loss: \"+ str(train_loss)+ \" L2 Loss: \"+ str(l2_train_loss))\n",
    "            sys.stdout.flush()\n",
    "        \n",
    "        \n",
    "        train_loss_mean = np.mean(train_loss_list)\n",
    "        train_accuracy_mean = np.mean(train_accuracy_list)\n",
    "        train_top5_mean = np.mean(train_top5_list)\n",
    "        l2_loss_mean = np.mean(l2_loss_list)\n",
    "        \n",
    "        plot_train_loss.append(train_loss_mean)\n",
    "        plot_train_accuracy.append(train_accuracy_mean)\n",
    "        plot_train_top5.append(train_top5_mean)\n",
    "        plot_l2_loss.append(l2_loss_mean)\n",
    "      \n",
    "        train_loss_list = []\n",
    "        train_accuracy_list = []\n",
    "        train_top5_list = []\n",
    "        l2_loss_list=[]\n",
    "        \n",
    "        print((7100 / BATCH_SIZE))\n",
    "        for i in xrange((7100/ BATCH_SIZE)):\n",
    "            val_loss, val_accuracy, val_top5 = sess.run(\n",
    "               [loss_tf, accuracy, accuracy_top5],feed_dict={train_mode: False})\n",
    "\n",
    "            val_loss_list.append(val_loss)\n",
    "            val_accuracy_list.append(val_accuracy)\n",
    "            val_top5_list.append(val_top5)\n",
    "            sys.stdout.write('\\r\\r' + \"Iteration:\" + str(i)+ \"  Loss: \"+ str(val_loss))\n",
    "            sys.stdout.flush()\n",
    "        \n",
    "        \n",
    "        val_loss_mean = np.mean(val_loss_list)\n",
    "        val_accuracy_mean = np.mean(val_accuracy_list)\n",
    "        val_top5_mean = np.mean(val_top5_list)\n",
    "        \n",
    "        plot_val_loss.append(val_loss_mean)\n",
    "        plot_val_accuracy.append(val_accuracy_mean)\n",
    "        plot_val_top5.append(val_top5_mean)\n",
    "      \n",
    "        val_loss_list = []\n",
    "        val_accuracy_list = []\n",
    "        val_top5_list = []        \n",
    "            \n",
    "\n",
    "        clear_output()\n",
    "        print \"===========**Training ACCURACY**================\"\n",
    "        print \"Epoch:\", epoch\n",
    "        print 'Training Top5 : ', train_top5_mean \n",
    "        print 'Training Accuracy: ', train_accuracy_mean       \n",
    "        print \"Training Loss:\", train_loss_mean \n",
    "        print \"L2 Loss:\", l2_loss_mean\n",
    "        \n",
    "        print \"===========**VALIDATION ACCURACY**================\"\n",
    "        print 'Epoch:', epoch\n",
    "        print 'Validation Top5: ', val_top5_mean\n",
    "        print 'Validation Accuracy: ', val_accuracy_mean\n",
    "        print \"Validation Loss:\", val_loss_mean\n",
    "        \n",
    "        print 'Time Elapsed for Epoch:' + str(epoch) + ' is ' + str(\n",
    "            (time.time() - epoch_start_time) / 60.) + ' minutes'\n",
    "\n",
    "        plt.figure(1) \n",
    "        aa = plt.plot(plot_l2_loss,'r')\n",
    "        plt.title(\"L2 LOSS\")\n",
    "        plt.figure(2) \n",
    "        aa = plt.plot(plot_train_loss,'r',label=\"Training\")\n",
    "        bb = plt.plot(plot_val_loss,'g',label=\"Validation\")\n",
    "        plt.title(\"Cross Entropy Loss\")\n",
    "        plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\n",
    "        plt.figure(3)\n",
    "        cc = plt.plot(plot_train_accuracy,'r',label=\"Training\")\n",
    "        dd = plt.plot(plot_val_accuracy,'g',label=\"Validation\")\n",
    "        plt.title(\"Top 1 Accuracy\")\n",
    "        plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\n",
    "        plt.figure(4)\n",
    "        cc = plt.plot(plot_train_top5,'r',label=\"Training\")\n",
    "        dd = plt.plot(plot_val_top5,'g',label=\"Validation\")\n",
    "        plt.title(\"Top 5 Accuracy\")\n",
    "        plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\n",
    "        plt.show()\n",
    "\n",
    "        \n",
    "        saver.save(sess, ckpt_dir + \"/gap-unrel2.ckpt\", global_step=epoch)\n",
    "        #vgg.save_npy(sess, '/mnt/data3/related_dataset/weights/vgg-rel-epoch-' + str(epoch) + '.npy')\n",
    "        ofile  = open('gap_unrel2.csv', \"a\")\n",
    "        writer = csv.writer(ofile)\n",
    "        timep=((time.time() - epoch_start_time) / 60.)\n",
    "        writer.writerow([train_loss_mean,train_accuracy_mean,train_top5_mean,val_loss_mean,val_accuracy_mean,val_top5_mean,l2_loss_mean,timep,epoch])\n",
    "        ofile.close()\n",
    "except tf.errors.OutOfRangeError:\n",
    "    print (\"out of range\")\n",
    "finally:\n",
    "    coord.request_stop()\n",
    "    coord.join(threads)\n",
    "    print(\"requesting stop\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vgg.save_npy(sess, '/mnt/data3/gap/unrel/npy/vgg-unrel2-gap-epoch-' + str(epoch) + '.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
