{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Simple tester for the vgg19_trainable\n",
    "\"\"\"\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "import vgg16_2classes as vgg16\n",
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "from IPython.display import clear_output\n",
    "import sys\n",
    "\n",
    "PRETRAINED_MODEL_PATH= \"/home/sik4hi/ckpt_dir\"\n",
    "N_EPOCHS = 100\n",
    "INIT_LEARNING_RATE = 0.01\n",
    "WEIGHT_DECAY_RATE = 0.0005\n",
    "MOMENTUM = 0.9\n",
    "IMAGE_HEIGHT  = 224    #960\n",
    "IMAGE_WIDTH   = 224    #720\n",
    "NUM_CHANNELS  = 3\n",
    "BATCH_SIZE = 100\n",
    "N_CLASSES = 1000\n",
    "DROPOUT = 0.50\n",
    "ckpt_dir = \"/home/sik4hi/ckpt_dir\"\n",
    "LOGS_PATH = '/home/sik4hi/tensorflow_logs'\n",
    "WEIGHT_PATH = '.npy'\n",
    "TRAINSET_PATH0 = '/mnt/data1/imagenet-data/csv-files/train2/imagenetdataall0.csv'\n",
    "TRAINSET_PATH1 = '/mnt/data1/imagenet-data/csv-files/train/imagenetdata1.csv'\n",
    "TRAINSET_PATH2 = '/mnt/data1/imagenet-data/csv-files/train/imagenetdata2.csv'\n",
    "TRAINSET_PATH3 = '/mnt/data1/imagenet-data/csv-files/train/imagenetdata3.csv'\n",
    "TRAINSET_PATH4 = '/mnt/data1/imagenet-data/csv-files/train/imagenetdata4.csv'\n",
    "TRAINSET_PATH5 = '/mnt/data1/imagenet-data/csv-files/train/imagenetdata5.csv'\n",
    "TRAINSET_PATH6 = '/mnt/data1/imagenet-data/csv-files/train/imagenetdata6.csv'\n",
    "TRAINSET_PATH7 = '/mnt/data1/imagenet-data/csv-files/train/imagenetdata7.csv'\n",
    "TRAINSET_PATH8 = '/mnt/data1/imagenet-data/csv-files/train/imagenetdata8.csv'\n",
    "TRAINSET_PATH9 = '/mnt/data1/imagenet-data/csv-files/train/imagenetdata9.csv'\n",
    "\n",
    "VALSET_PATH0 ='/mnt/data1/imagenet-data/csv-files/val/imagenetdata0.csv'\n",
    "VALSET_PATH1 ='/mnt/data1/imagenet-data/csv-files/val/imagenetdata1.csv'\n",
    "VALSET_PATH2 ='/mnt/data1/imagenet-data/csv-files/val/imagenetdata2.csv'\n",
    "VALSET_PATH3 ='/mnt/data1/imagenet-data/csv-files/val/imagenetdata3.csv'\n",
    "VALSET_PATH4 ='/mnt/data1/imagenet-data/csv-files/val/imagenetdata4.csv'\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"\"\n",
    "f=open(TRAINSET_PATH0,\"r\")\n",
    "filepaths=[]\n",
    "labels=[]\n",
    "for line in f:\n",
    "    filepath, label= line.split(\",\")\n",
    "    label= int(label)\n",
    "    filepaths.append(filepath)\n",
    "    labels.append(label)\n",
    "\n",
    "with tf.device('/cpu:0'):\n",
    "    one_ex = tf.train.slice_input_producer([filepaths,labels#, TRAINSET_PATH1\n",
    "                                              #, TRAINSET_PATH2, TRAINSET_PATH3,\n",
    "                                              # TRAINSET_PATH4, TRAINSET_PATH5\n",
    "                                              #, TRAINSET_PATH6, TRAINSET_PATH7, \n",
    "                                              # TRAINSET_PATH8, TRAINSET_PATH9\n",
    "                                              ],capacity = 1281144, shuffle=True)\n",
    "    \n",
    "    \n",
    "    im_content = tf.read_file(one_ex[0])\n",
    "    train_image = tf.image.decode_jpeg(im_content, channels=3)\n",
    "    \n",
    "    # train_image = augment(train_image)\n",
    "    #size = tf.cast([IMAGE_HEIGHT, IMAGE_WIDTH], tf.int32)\n",
    "    #train_image = tf.image.resize_images(train_image, size)\n",
    "    train_label = tf.cast(one_ex[1], tf.int64) # unnecessary\n",
    "    #train_label = one_ex[1]\n",
    "    def _compute_longer_edge(height, width, new_shorter_edge):\n",
    "        return tf.cast(width*new_shorter_edge/height, tf.int32)\n",
    "\n",
    "    shape = tf.shape(train_image)\n",
    "    height = shape[0]\n",
    "    width = shape[1]\n",
    "    new_shorter_edge = tf.constant(256, dtype=tf.int32)\n",
    "\n",
    "    height_smaller_than_width = tf.less_equal(height, width)\n",
    "    new_height_and_width = tf.cond(\n",
    "        height_smaller_than_width,\n",
    "        lambda: (new_shorter_edge, _compute_longer_edge(height, width, new_shorter_edge)),\n",
    "        lambda: (_compute_longer_edge(width, height, new_shorter_edge), new_shorter_edge)\n",
    "    )\n",
    "    size = tf.cast([new_height_and_width[0], new_height_and_width[1]], tf.int32)\n",
    "    train_image = tf.image.resize_images(train_image, size)\n",
    "    size = tf.cast([IMAGE_HEIGHT, IMAGE_WIDTH, 3], tf.int32)\n",
    "    train_image = tf.random_crop(train_image, size)\n",
    "    train_image = tf.image.random_flip_left_right(train_image)\n",
    "    train_image = tf.cast(train_image, tf.float32)/255. # necessary for mapping rgb channels from 0-255 to 0-1 float.\n",
    "    #train_label_batch = tf.one_hot(train_label_batch, 1000)\n",
    "    train_image_batch, train_label_batch = tf.train.shuffle_batch([train_image, train_label], batch_size=BATCH_SIZE,\n",
    "                                                                  capacity = 1000 + 3 * BATCH_SIZE, \n",
    "                                                                  min_after_dequeue = 1000)\n",
    "with tf.device('/cpu:0'):\n",
    "    sess = tf.Session()\n",
    "    init_op = tf.group(tf.initialize_all_variables(),\n",
    "                       tf.initialize_local_variables())\n",
    "    sess.run(init_op)\n",
    "    #print(csv_path)\n",
    "    # For populating queues with batches, very important!\n",
    "    threads = tf.train.start_queue_runners(sess=sess)\n",
    "    print(\"done\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[868  21 797 800 495 556 746 720  91 451 527 784 362 437 919 690 397 461\n",
      " 565 167 757 680 543 130 855 165 129 632 868 147 284 887 323 685 675 962\n",
      " 856 665 108 783 476 170 336 780 462 207 936 892 861 312 903   1 341 299\n",
      " 334 815  75 565 875 776 779 433 803 803 758 200 949 410 430 759 879 739\n",
      " 972 537 371 640 704 795   1 954 771 838 337 414 411 424 688 661 386 793\n",
      " 702 874 982 626 392 898 903  35 855 766]\n",
      "Time Elapsed for Epoch: is 0.447953414917 minutes\n"
     ]
    }
   ],
   "source": [
    "epoch_start_time = time.time()\n",
    "train_imbatch, train_labatch = sess.run([train_image_batch, train_label_batch])\n",
    "print(train_labatch)\n",
    "print 'Time Elapsed for Epoch:'+ ' is ' + str(\n",
    "            (time.time() - epoch_start_time) / 60.) + ' minutes'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "TRAINSET_PATH0 = '/mnt/data1/imagenet-data/csv-files/train2/imagenetdataall0.csv'\n",
    "f=open(TRAINSET_PATH0,\"r\")\n",
    "filepaths=[]\n",
    "labels=[]\n",
    "for line in f:\n",
    "    filepath, label= line.split(\",\")\n",
    "    label= int(label)\n",
    "    filepaths.append(filepath)\n",
    "    labels.append(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "937\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
