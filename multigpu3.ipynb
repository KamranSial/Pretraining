{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "import vgg16_modified as vgg16\n",
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "from IPython.display import clear_output\n",
    "import sys\n",
    "#os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "PRETRAINED_MODEL_PATH= \"/home/sik4hi/ckpt_dir\"\n",
    "N_EPOCHS = 300\n",
    "INIT_LEARNING_RATE = 0.01\n",
    "WEIGHT_DECAY_RATE = 0.0005\n",
    "MOMENTUM = 0.9\n",
    "IMAGE_HEIGHT  = 224    #960\n",
    "IMAGE_WIDTH   = 224    #720\n",
    "NUM_CHANNELS  = 3\n",
    "BATCH_SIZE = 180\n",
    "N_CLASSES = 1000\n",
    "DROPOUT = 0.50\n",
    "NUM_GPUS=2\n",
    "ckpt_dir = \"/home/sik4hi/ckpt_dir\"\n",
    "LOGS_PATH = '/home/sik4hi/tensorflow_logs'\n",
    "WEIGHT_PATH = '.npy'\n",
    "TRAINSET_PATH0 = '/mnt/data1/imagenet-data/csv-files/train2/imagenetdata0.csv'\n",
    "TRAINSET_PATH1 = '/mnt/data1/imagenet-data/csv-files/train/imagenetdata1.csv'\n",
    "TRAINSET_PATH2 = '/mnt/data1/imagenet-data/csv-files/train/imagenetdata2.csv'\n",
    "TRAINSET_PATH3 = '/mnt/data1/imagenet-data/csv-files/train/imagenetdata3.csv'\n",
    "TRAINSET_PATH4 = '/mnt/data1/imagenet-data/csv-files/train/imagenetdata4.csv'\n",
    "TRAINSET_PATH5 = '/mnt/data1/imagenet-data/csv-files/train/imagenetdata5.csv'\n",
    "TRAINSET_PATH6 = '/mnt/data1/imagenet-data/csv-files/train/imagenetdata6.csv'\n",
    "TRAINSET_PATH7 = '/mnt/data1/imagenet-data/csv-files/train/imagenetdata7.csv'\n",
    "TRAINSET_PATH8 = '/mnt/data1/imagenet-data/csv-files/train/imagenetdata8.csv'\n",
    "TRAINSET_PATH9 = '/mnt/data1/imagenet-data/csv-files/train/imagenetdata9.csv'\n",
    "\n",
    "VALSET_PATH0 ='/mnt/data1/imagenet-data/csv-files/val/imagenetdata100-0.csv'\n",
    "VALSET_PATH1 ='/mnt/data1/imagenet-data/csv-files/val/imagenetdata1.csv'\n",
    "VALSET_PATH2 ='/mnt/data1/imagenet-data/csv-files/val/imagenetdata2.csv'\n",
    "VALSET_PATH3 ='/mnt/data1/imagenet-data/csv-files/val/imagenetdata3.csv'\n",
    "VALSET_PATH4 ='/mnt/data1/imagenet-data/csv-files/val/imagenetdata4.csv'\n",
    "\n",
    "\n",
    "def _tower_loss(images, labels,train_mode, scope):\n",
    "    # Build inference Graph.\n",
    "    \n",
    "    vgg = vgg16.Vgg16()\n",
    "    vgg.build(images, train_mode)\n",
    "    weights_only = filter(lambda x: x.name.endswith('W:0'), tf.trainable_variables())\n",
    "    for x in xrange(len(weights_only)):\n",
    "        print (weights_only[x].name)\n",
    "    # print number of variables used: 143667240 variables, i.e. ideal size = 548MB\n",
    "    print vgg.get_var_count()\n",
    "    \n",
    "    # Build the portion of the Graph calculating the losses. Note that we will\n",
    "    # assemble the total_loss using a custom function below.\n",
    "    loss_tf = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(vgg.prob, labels), name='loss_tf')\n",
    "    #loss_summary = tf.summary.scalar(\"loss\", loss_tf)\n",
    "    weights_only = filter( lambda x: x.name.endswith('W:0'), tf.trainable_variables())\n",
    "    weight_decay = tf.reduce_mean(tf.add_n([tf.nn.l2_loss(x) for x in weights_only])) * WEIGHT_DECAY_RATE\n",
    "    total_loss = loss_tf + weight_decay\n",
    "    \n",
    "    #correct_pred = tf.equal(tf.argmax(vgg.prob, 1), labels)\n",
    "    #accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "    \n",
    "    return total_loss,loss_tf, accuracy\n",
    "\n",
    "\n",
    "def average_gradients(tower_grads):\n",
    "    average_grads = []\n",
    "    for grad_and_vars in zip(*tower_grads):\n",
    "    # Note that each grad_and_vars looks like the following:\n",
    "    #   ((grad0_gpu0, var0_gpu0), ... , (grad0_gpuN, var0_gpuN))\n",
    "        grads = []\n",
    "        for g, _ in grad_and_vars:\n",
    "        # Add 0 dimension to the gradients to represent the tower.\n",
    "            expanded_g = tf.expand_dims(g, 0)\n",
    "\n",
    "            # Append on a 'tower' dimension which we will average over below.\n",
    "            grads.append(expanded_g)\n",
    "\n",
    "        # Average over the 'tower' dimension.\n",
    "        grad = tf.concat(0,grads)\n",
    "        grad = tf.reduce_mean(grad, 0)\n",
    "\n",
    "        # Keep in mind that the Variables are redundant because they are shared\n",
    "        # across towers. So .. we will just return the first tower's pointer to\n",
    "        # the Variable.\n",
    "        v = grad_and_vars[0][1]\n",
    "        grad_and_var = (grad, v)\n",
    "        average_grads.append(grad_and_var)\n",
    "    return average_grads\n",
    "\n",
    "def _compute_longer_edge(height, width, new_shorter_edge):\n",
    "    return tf.cast(width*new_shorter_edge/height, tf.int32)\n",
    "    \n",
    "\n",
    "    \"\"\"Train CIFAR-10 for a number of steps.\"\"\"\n",
    "with tf.Graph().as_default(), tf.device('/cpu:0'):\n",
    "    learning_rate = tf.placeholder(tf.float32, [])\n",
    "    images_tf = tf.placeholder(tf.float32, [BATCH_SIZE, 224, 224, 3])\n",
    "    labels_tf = tf.placeholder(tf.int64,[BATCH_SIZE])\n",
    "    train_mode = tf.placeholder(tf.bool)\n",
    "        \n",
    "    opt = tf.train.AdamOptimizer(0.01,epsilon=0.1)\n",
    "       \n",
    "    csv_path = tf.train.string_input_producer([TRAINSET_PATH0\n",
    "                                               #, TRAINSET_PATH1\n",
    "                                              #, TRAINSET_PATH2, TRAINSET_PATH3,\n",
    "                                               #TRAINSET_PATH4, TRAINSET_PATH5\n",
    "                                              #, TRAINSET_PATH6, TRAINSET_PATH7, \n",
    "                                              # TRAINSET_PATH8, TRAINSET_PATH\n",
    "                                              ], shuffle=True)\n",
    "    textReader = tf.TextLineReader()\n",
    "    _,csv_content = textReader.read(csv_path)\n",
    "    im_name, im_label = tf.decode_csv(csv_content, record_defaults=[[\"\"], [1]])\n",
    "\n",
    "    im_content = tf.read_file(im_name)\n",
    "    train_image = tf.image.decode_jpeg(im_content, channels=3)\n",
    "    train_image = tf.cast(train_image, tf.float32)/255. # necessary for mapping rgb channels from 0-255 to 0-1 float.\n",
    "    # train_image = augment(train_image)\n",
    "    size = tf.cast([IMAGE_HEIGHT, IMAGE_WIDTH], tf.int32)\n",
    "    train_image = tf.image.resize_images(train_image, size)\n",
    "    train_label = tf.cast(im_label, tf.int64) # unnecessary\n",
    "    train_image_batch, train_label_batch = tf.train.shuffle_batch([train_image, train_label], batch_size=BATCH_SIZE,\n",
    "                                                                      capacity = 300 + 3*BATCH_SIZE, \n",
    "                                                                  min_after_dequeue = 300)\n",
    "    \n",
    "    \n",
    "    images_splits = tf.split(0, NUM_GPUS, train_image_batch)\n",
    "    labels_splits = tf.split(0, NUM_GPUS, train_label_batch)\n",
    "        \n",
    "        # Calculate the gradients for each model tower.\n",
    "    tower_grads = []\n",
    "    #tower_accuracy = []\n",
    "    for i in xrange(NUM_GPUS):\n",
    "        with tf.device('/gpu:%d' % i):\n",
    "            with tf.name_scope('%s_%d' % ('tower', i)) as scope:\n",
    "                # Calculate the loss for one tower of the CIFAR model. This function\n",
    "                # constructs the entire CIFAR model but shares the variables across\n",
    "                # all towers.\n",
    "                    \n",
    "                loss_tf2, loss_tf, accuracy = _tower_loss(images_splits[i], labels_splits[i],train_mode, scope)\n",
    "                # Reuse variables for the next tower.\n",
    "                tf.get_variable_scope().reuse_variables()\n",
    "\n",
    "                # Retain the summaries from the final tower.\n",
    "                # summaries = tf.get_collection(tf.GraphKeys.SUMMARIES, scope)\n",
    "\n",
    "                # Calculate the gradients for the batch of data on this CIFAR tower.\n",
    "                grads = opt.compute_gradients(loss_tf2)\n",
    "                    \n",
    "                # Keep track of the gradients across all towers.\n",
    "                tower_grads.append(grads)\n",
    "                #tower_accuracy.append(accuracy)\n",
    "    # We must calculate the mean of each gradient. Note that this is the\n",
    "    # synchronization point across all towers.\n",
    "    grads = average_gradients(tower_grads)\n",
    "    total_accuracy=tf.reduce_mean(tower_accuracy)\n",
    "    # Add a summary to track the learning rate.\n",
    "    # summaries.append(tf.contrib.deprecated.scalar_summary('learning_rate', lr))\n",
    "\n",
    "    # Add histograms for gradients.\n",
    "    #for grad, var in grads:\n",
    "    #      if grad is not None:\n",
    "    #            summaries.append(\n",
    "    #            tf.contrib.deprecated.histogram_summary(var.op.name + '/gradients',\n",
    "    #                                            grad))\n",
    "\n",
    "    # Apply the gradients to adjust the shared variables.\n",
    "    apply_gradient_op = opt.apply_gradients(grads)\n",
    "\n",
    "    # Add histograms for trainable variables.\n",
    "    #for var in tf.trainable_variables():\n",
    "    #      summaries.append(tf.contrib.deprecated.histogram_summary(var.op.name, var))\n",
    "\n",
    "    # Track the moving averages of all trainable variables.\n",
    "    #variable_averages = tf.train.ExponentialMovingAverage(cifar10.MOVING_AVERAGE_DECAY, global_step)\n",
    "    #variables_averages_op = variable_averages.apply(tf.trainable_variables())\n",
    "\n",
    "    # Group all updates to into a single train op.\n",
    "    #train_op = tf.group(apply_gradient_op, variables_averages_op)\n",
    "    train_op = apply_gradient_op\n",
    "                            \n",
    "    # Create a saver.\n",
    "    if not os.path.exists(ckpt_dir):\n",
    "        os.makedirs(ckpt_dir)\n",
    "    saver = tf.train.Saver(max_to_keep=5)\n",
    "    # Build the summary operation from the last tower summaries.\n",
    "    #summary_op = tf.contrib.deprecated.merge_summary(summaries)\n",
    "\n",
    "    # Build an initialization operation to run below.\n",
    "    init= tf.group(tf.initialize_all_variables(),\n",
    "                    tf.initialize_local_variables())\n",
    "\n",
    "    # Start running operations on the Graph. allow_soft_placement must be set to\n",
    "    # True to build towers on GPU, as some of the ops do not have GPU\n",
    "    # implementations.\n",
    "    \n",
    "    \n",
    "    config=tf.ConfigProto(\n",
    "            allow_soft_placement=True,\n",
    "            log_device_placement=False)\n",
    "    #config.gpu_options.allow_growth=True\n",
    "    #config=tf.GPUOptions(allow_growth=True)\n",
    "    #config += tf.GPUOptions(allow_growth=True) \n",
    "    sess = tf.Session(config=config)\n",
    "    sess.run(init)\n",
    "    # Start the queue runners.\n",
    "    tf.train.start_queue_runners(sess=sess)\n",
    "\n",
    "    #summary_writer = tf.summary.FileWriter(FLAGS.train_dir, sess.graph)\n",
    "\n",
    "        \n",
    "    loss_list, train_list, plot_loss, plot_acc , loss_list2 , val_list ,plot_loss2, plot_acc2, total_loss_list1, total_loss_list2, plot_ttloss, plot_tvloss  = [], [], [], [], [], [], [], [], [], [], [], []\n",
    "    #summary_writer = tf.summary.FileWriter(LOGS_PATH, graph=tf.get_default_graph())\n",
    "    steps = 1\n",
    "    count = 1\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for epoch in xrange(N_EPOCHS):\n",
    "    train_correct = 0\n",
    "    train_data = 0\n",
    "    epoch_start_time = time.time()\n",
    "    print((1281144 / BATCH_SIZE))\n",
    "    #print((2600 / BATCH_SIZE) + 1)\n",
    "    for i in xrange((1281144/ BATCH_SIZE)):\n",
    "        #train_imbatch, train_labatch = sess.run([train_image_batch, train_label_batch])\n",
    "        _, train_loss,total_loss, train_accuracy = sess.run(\n",
    "                    [train_op, loss_tf, loss_tf2, accuracy],\n",
    "                    feed_dict={learning_rate: INIT_LEARNING_RATE, images_tf: train_imbatch, labels_tf:\n",
    "                               train_labatch, train_mode: True})\n",
    "\n",
    "        loss_list.append(train_loss)\n",
    "        train_list.append(train_accuracy)\n",
    "        total_loss_list1.append(total_loss)\n",
    "        sys.stdout.write('\\r' + 'iteration:' + str(i))\n",
    "        sys.stdout.flush()\n",
    "\n",
    "            \n",
    "    clear_output()\n",
    "    t = np.mean(train_list)\n",
    "    l = np.mean(loss_list)\n",
    "    ttl = np.mean(total_loss_list1)\n",
    "    print \"===========**Training ACCURACY**================\"\n",
    "    print \"Epoch\", epoch + 1#, \"Iteration\", steps\n",
    "    print 'Training Accuracy: ', t\n",
    "    print \"Training Loss:\", l \n",
    "    print \"Total Training Loss:\", ttl\n",
    "                \n",
    "    plot_loss.append(l)\n",
    "    plot_acc.append(t)\n",
    "    plot_ttloss.append(ttl)\n",
    "      \n",
    "      \n",
    "                #summary_writer.add_summary(summary_str, steps)\n",
    "    loss_list = []\n",
    "    train_list = []\n",
    "    total_loss_list1=[]\n",
    "                \n",
    "            \n",
    "            #INIT_LEARNING_RATE *= 0.99\n",
    "    print((50000 / BATCH_SIZE))\n",
    "    for i in xrange((50000 / BATCH_SIZE) ):\n",
    "        val_imbatch, val_labatch = sess.run([val_image_batch, val_label_batch])\n",
    "        val_accuracy, val_loss, val_tloss = sess.run([accuracy, loss_tf, loss_tf2], feed_dict={images_tf: val_imbatch, labels_tf: val_labatch, train_mode: False})\n",
    "        loss_list2.append(val_loss)\n",
    "        val_list.append(val_accuracy)\n",
    "        total_loss_list2.append(val_tloss)\n",
    "        sys.stdout.write('\\r' + 'iteration:' + str(i))\n",
    "        sys.stdout.flush()\n",
    "            \n",
    "    t = np.mean(val_list)\n",
    "    l = np.mean(loss_list2)\n",
    "    tvl = np.mean(total_loss_list2)\n",
    "    # #f_log.write('epoch:' + str(epoch + 1) + '\\tacc:' + str(val_accuracy) + '\\n')\n",
    "    print \"===========**VALIDATION ACCURACY**================\"\n",
    "    print 'Epoch:' + str(epoch + 1)# + '\\tacc:' + str(val_accuracy) + '\\n'\n",
    "    print 'Validation Accuracy: ', t\n",
    "    #print 'labels: ', train_labatch\n",
    "    print \"Validation Loss:\", l \n",
    "    print \"Total Validation Loss:\", tvl\n",
    "    print 'Time Elapsed for Epoch:' + str(epoch + 1) + ' is ' + str(\n",
    "            (time.time() - epoch_start_time) / 60.) + ' minutes'\n",
    "    plot_loss2.append(l)\n",
    "    plot_acc2.append(t)\n",
    "    plot_tvloss.append(tvl)\n",
    "        \n",
    "    plt.figure(1) \n",
    "    #aa = plt.plot(plot_ttloss,'r',label=\"Training\")\n",
    "    bb = plt.plot(plot_tvloss,'g',label=\"Validation\")\n",
    "    plt.title(\"TOTAL LOSS\")\n",
    "    plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\n",
    "    plt.figure(2) \n",
    "    aa = plt.plot(plot_loss,'r',label=\"Training\")\n",
    "    bb = plt.plot(plot_loss2,'g',label=\"Validation\")\n",
    "    plt.title(\"LOSS\")\n",
    "    plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\n",
    "    plt.figure(3)\n",
    "    #cc = plt.plot(plot_acc,'r',label=\"Training\")\n",
    "    dd = plt.plot(plot_acc2,'g',label=\"Validation\")\n",
    "    plt.title(\"Accuracy\")\n",
    "    plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\n",
    "    plt.show()\n",
    "    loss_list2 = []\n",
    "    val_list = []\n",
    "    total_loss_list2=[]\n",
    "    #if (epoch % 1 == 0):            \n",
    "    #    saver.save(sess, ckpt_dir + \"/model.ckpt\", global_step=epoch)\n",
    "    # test savel\n",
    "    #vgg.save_npy(sess, './test-save'+str(epoch)+'.npy')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
