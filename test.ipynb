{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Simple tester for the vgg19_trainable\n",
    "\"\"\"\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "import vgg16_2classes as vgg16\n",
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "from IPython.display import clear_output\n",
    "import sys\n",
    "\n",
    "PRETRAINED_MODEL_PATH= \"/home/sik4hi/ckpt_dir\"\n",
    "N_EPOCHS = 100\n",
    "INIT_LEARNING_RATE = 0.01\n",
    "WEIGHT_DECAY_RATE = 0.0005\n",
    "MOMENTUM = 0.9\n",
    "IMAGE_HEIGHT  = 224    #960\n",
    "IMAGE_WIDTH   = 224    #720\n",
    "NUM_CHANNELS  = 3\n",
    "BATCH_SIZE = 100\n",
    "N_CLASSES = 1000\n",
    "DROPOUT = 0.50\n",
    "ckpt_dir = \"/home/sik4hi/ckpt_dir\"\n",
    "LOGS_PATH = '/home/sik4hi/tensorflow_logs'\n",
    "WEIGHT_PATH = '.npy'\n",
    "TRAINSET_PATH0 = '/mnt/data1/imagenet-data/csv-files/train2/imagenetdataall0.csv'\n",
    "TRAINSET_PATH1 = '/mnt/data1/imagenet-data/csv-files/train/imagenetdata1.csv'\n",
    "TRAINSET_PATH2 = '/mnt/data1/imagenet-data/csv-files/train/imagenetdata2.csv'\n",
    "TRAINSET_PATH3 = '/mnt/data1/imagenet-data/csv-files/train/imagenetdata3.csv'\n",
    "TRAINSET_PATH4 = '/mnt/data1/imagenet-data/csv-files/train/imagenetdata4.csv'\n",
    "TRAINSET_PATH5 = '/mnt/data1/imagenet-data/csv-files/train/imagenetdata5.csv'\n",
    "TRAINSET_PATH6 = '/mnt/data1/imagenet-data/csv-files/train/imagenetdata6.csv'\n",
    "TRAINSET_PATH7 = '/mnt/data1/imagenet-data/csv-files/train/imagenetdata7.csv'\n",
    "TRAINSET_PATH8 = '/mnt/data1/imagenet-data/csv-files/train/imagenetdata8.csv'\n",
    "TRAINSET_PATH9 = '/mnt/data1/imagenet-data/csv-files/train/imagenetdata9.csv'\n",
    "\n",
    "VALSET_PATH0 ='/mnt/data1/imagenet-data/csv-files/val/imagenetdata0.csv'\n",
    "VALSET_PATH1 ='/mnt/data1/imagenet-data/csv-files/val/imagenetdata1.csv'\n",
    "VALSET_PATH2 ='/mnt/data1/imagenet-data/csv-files/val/imagenetdata2.csv'\n",
    "VALSET_PATH3 ='/mnt/data1/imagenet-data/csv-files/val/imagenetdata3.csv'\n",
    "VALSET_PATH4 ='/mnt/data1/imagenet-data/csv-files/val/imagenetdata4.csv'\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"\"\n",
    "#=======================================================================================================\n",
    "# Reading Training data from CSV FILE\n",
    "#=======================================================================================================\n",
    "with tf.device('/cpu:0'):\n",
    "    csv_path = tf.train.string_input_producer([TRAINSET_PATH0#, TRAINSET_PATH1\n",
    "                                              #, TRAINSET_PATH2, TRAINSET_PATH3,\n",
    "                                              # TRAINSET_PATH4, TRAINSET_PATH5\n",
    "                                              #, TRAINSET_PATH6, TRAINSET_PATH7, \n",
    "                                              # TRAINSET_PATH8, TRAINSET_PATH9\n",
    "                                              ], shuffle=True)\n",
    "    textReader = tf.TextLineReader()\n",
    "    _, csv_content = textReader.read(csv_path)\n",
    "    im_name, im_label = tf.decode_csv(csv_content, record_defaults=[[\"\"], [1]])\n",
    "\n",
    "    im_content = tf.read_file(im_name)\n",
    "    train_image = tf.image.decode_jpeg(im_content, channels=3)\n",
    "    \n",
    "    # train_image = augment(train_image)\n",
    "    #size = tf.cast([IMAGE_HEIGHT, IMAGE_WIDTH], tf.int32)\n",
    "    #train_image = tf.image.resize_images(train_image, size)\n",
    "    train_label = tf.cast(im_label, tf.int64) # unnecessary\n",
    "\n",
    "    def _compute_longer_edge(height, width, new_shorter_edge):\n",
    "        return tf.cast(width*new_shorter_edge/height, tf.int32)\n",
    "\n",
    "    shape = tf.shape(train_image)\n",
    "    height = shape[0]\n",
    "    width = shape[1]\n",
    "    new_shorter_edge = tf.constant(256, dtype=tf.int32)\n",
    "\n",
    "    height_smaller_than_width = tf.less_equal(height, width)\n",
    "    new_height_and_width = tf.cond(\n",
    "        height_smaller_than_width,\n",
    "        lambda: (new_shorter_edge, _compute_longer_edge(height, width, new_shorter_edge)),\n",
    "        lambda: (_compute_longer_edge(width, height, new_shorter_edge), new_shorter_edge)\n",
    "    )\n",
    "    size = tf.cast([new_height_and_width[0], new_height_and_width[1]], tf.int32)\n",
    "    train_image = tf.image.resize_images(train_image, size)\n",
    "    size = tf.cast([IMAGE_HEIGHT, IMAGE_WIDTH, 3], tf.int32)\n",
    "    train_image = tf.random_crop(train_image, size)\n",
    "    train_image = tf.image.random_flip_left_right(train_image)\n",
    "    train_image = tf.cast(train_image, tf.float32)/255. # necessary for mapping rgb channels from 0-255 to 0-1 float.\n",
    "    #train_label_batch = tf.one_hot(train_label_batch, 1000)\n",
    "    train_image_batch, train_label_batch = tf.train.shuffle_batch([train_image, train_label], batch_size=BATCH_SIZE,\n",
    "                                                                  capacity = 10000 + 3 * BATCH_SIZE, \n",
    "                                                                  min_after_dequeue = 1000)\n",
    "\n",
    "    \n",
    "\n",
    "with tf.device('/cpu:0'):\n",
    "   \n",
    "    \n",
    "    sess = tf.Session()\n",
    "    learning_rate = tf.placeholder(tf.float32, [])\n",
    "    images_tf = tf.placeholder(tf.float32, [None, 224, 224, 3])\n",
    "    labels_tf = tf.placeholder(tf.int64)\n",
    "    train_mode = tf.placeholder(tf.bool)\n",
    "\n",
    "    vgg = vgg16.Vgg16()\n",
    "    vgg.build(images_tf, train_mode)\n",
    "    weights_only = filter(lambda x: x.name.endswith('W:0'), tf.trainable_variables())\n",
    "    for x in xrange(len(weights_only)):\n",
    "        print (weights_only[x].name)\n",
    "    # print number of variables used: 143667240 variables, i.e. ideal size = 548MB\n",
    "    print vgg.get_var_count()\n",
    "\n",
    "\n",
    "    #==============================================================================================================\n",
    "    # Defining Loss, could be changed from cross entropy depending on needs. The current configuration works well on\n",
    "    # multiclass (not hot-encoded vectors) prediction like ImageNET.\n",
    "    #==============================================================================================================\n",
    "    with tf.name_scope('Loss'):\n",
    "        #weight_decay = WEIGHT_DECAY_RATE * tf.add_n([tf.nn.l2_loss(x) for x in weights_only])\n",
    "        loss_tf = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(vgg.fc8, labels_tf), name='loss_tf')\n",
    "                  #+weight_decay\n",
    "        #loss_summary = tf.summary.scalar(\"loss\", loss_tf)\n",
    "        #weights_only = filter( lambda x: x.name.endswith('W:0'), tf.trainable_variables())\n",
    "        #weight_decay = tf.reduce_mean(tf.add_n([tf.nn.l2_loss(x) for x in weights_only]))* WEIGHT_DECAY_RATE \n",
    "        #loss_tf2 =loss_tf + weight_decay\n",
    "\n",
    "    # ==============================================================================================================\n",
    "    # Optimizer, again it can be changed to any function provided by Tensorflow. You can simply use commented out line\n",
    "    # instead of explicitly computing gradients, if you are not interested in creating summaries of gradients.\n",
    "    # ==============================================================================================================\n",
    "    train_op = tf.train.MomentumOptimizer(learning_rate, MOMENTUM).minimize(loss_tf)\n",
    "    #train_op = tf.train.AdamOptimizer(learning_rate=0.01,epsilon=0.1).minimize(loss_tf)\n",
    "\n",
    "    ##optimizer = tf.train.MomentumOptimizer(learning_rate, MOMENTUM)\n",
    "    #grads_and_vars = optimizer.compute_gradients(loss_tf)\n",
    "    #grads_and_vars = map(\n",
    "     #   lambda gv: (gv[0], gv[1]) if ('conv6' in gv[1].name or 'GAP' in gv[1].name) else (gv[0] * 0.1, gv[1]),\n",
    "      #  grads_and_vars)\n",
    "    #grads_and_vars = [(tf.clip_by_value(gv[0], -5., 5.), gv[1]) for gv in grads_and_vars]\n",
    "    #train_op = optimizer.apply_gradients(grads_and_vars)\n",
    "    #===================================================================================================================\n",
    "    # Summaries for the gradients\n",
    "    #===================================================================================================================\n",
    "    #for var in tf.trainable_variables():\n",
    "     #   tf.summary.histogram(var.op.name, var)\n",
    "    #summary_op = tf.summary.merge_all()\n",
    "\n",
    "\n",
    "    # ===================================================================================================================\n",
    "    # Accuracy for the current batch\n",
    "    # ===================================================================================================================\n",
    "    correct_pred = tf.equal(tf.argmax(vgg.prob, 1), labels_tf)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "\n",
    "\n",
    "with tf.device('/cpu:0'):\n",
    "    \n",
    "    init_op = tf.group(tf.initialize_all_variables(),\n",
    "                       tf.initialize_local_variables())\n",
    "    sess.run(init_op)\n",
    "    #print(csv_path)\n",
    "    # For populating queues with batches, very important!\n",
    "    threads = tf.train.start_queue_runners(sess=sess)\n",
    "\n",
    "    loss_list, train_list, plot_loss, plot_acc , loss_list2 , val_list ,plot_loss2, plot_acc2, tloss_list, plot_tloss = [], [], [], [], [], [], [], [], [], []\n",
    "    #summary_writer = tf.summary.FileWriter(LOGS_PATH, graph=tf.get_default_graph())\n",
    "    steps = 1\n",
    "    count = 1\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for epoch in xrange(N_EPOCHS):\n",
    "\n",
    "        train_correct = 0\n",
    "        train_data = 0\n",
    "        epoch_start_time = time.time()\n",
    "        print((2600 / BATCH_SIZE))\n",
    "        #print((2600 / BATCH_SIZE) + 1)\n",
    "        for i in xrange((100/ BATCH_SIZE)):\n",
    "            train_imbatch, train_labatch = sess.run([train_image_batch, train_label_batch])\n",
    "            _, train_loss,total_loss, output_val, train_accuracy = sess.run(\n",
    "               [train_op, loss_tf, loss_tf2, vgg.prob, accuracy],\n",
    "               feed_dict={learning_rate: INIT_LEARNING_RATE, images_tf: train_imbatch, labels_tf:\n",
    "               train_labatch, train_mode: True})\n",
    "\n",
    "            loss_list.append(train_loss)\n",
    "            train_list.append(train_accuracy)\n",
    "            tloss_list.append(total_loss)\n",
    "            sys.stdout.write('\\r' + 'iteration:' + str(i))\n",
    "            sys.stdout.flush()\n",
    "            #train_data += len(output_val)\n",
    "            \n",
    "\n",
    "        clear_output()\n",
    "        t = np.mean(train_list)\n",
    "        l = np.mean(loss_list)\n",
    "        tl= np.mean(tloss_list)\n",
    "        print \"===========**Training ACCURACY**================\"\n",
    "        print \"Epoch\", epoch + 1#, \"Iteration\", steps\n",
    "        #print \"Processed\", train_data, '/', 7800  # (count*BATCH_SIZE)\n",
    "        print 'Training Accuracy: ', t\n",
    "        #print 'labels: ', train_labatch\n",
    "        print \"Training Loss:\", l \n",
    "                \n",
    "        plot_loss.append(l)\n",
    "        plot_acc.append(t)\n",
    "        plot_tloss.append(tl)\n",
    "      \n",
    "      \n",
    "                #summary_writer.add_summary(summary_str, steps)\n",
    "        loss_list = []\n",
    "        train_list = []\n",
    "                \n",
    "            \n",
    "        #INIT_LEARNING_RATE *= 0.99\n",
    "\n",
    "#        for i in xrange((100 / BATCH_SIZE) ):\n",
    "#            val_imbatch, val_labatch = sess.run([val_image_batch, val_label_batch])\n",
    "#            val_accuracy, val_loss = sess.run([accuracy, loss_tf], feed_dict={images_tf: val_imbatch, labels_tf: val_labatch, train_mode: False})\n",
    "#            loss_list2.append(val_loss)\n",
    "#            val_list.append(val_accuracy)\n",
    "#            sys.stdout.write('\\r' + 'iteration:' + str(i))\n",
    "#            sys.stdout.flush()\n",
    "            \n",
    "#        t = np.mean(val_list)\n",
    "#        l = np.mean(loss_list2)\n",
    "        # #f_log.write('epoch:' + str(epoch + 1) + '\\tacc:' + str(val_accuracy) + '\\n')\n",
    "#        print \"===========**VALIDATION ACCURACY**================\"\n",
    "#        print 'epoch:' + str(epoch + 1)# + '\\tacc:' + str(val_accuracy) + '\\n'\n",
    "#        print 'Validation Accuracy: ', t\n",
    "        #print 'labels: ', train_labatch\n",
    "#        print \"Validation Loss:\", l \n",
    "        print 'Time Elapsed for Epoch:' + str(epoch + 1) + ' is ' + str(\n",
    "            (time.time() - epoch_start_time) / 60.) + ' minutes'\n",
    "#        plot_loss2.append(l)\n",
    "#        plot_acc2.append(t)\n",
    "        plt.figure(1)\n",
    "        cc = plt.plot(plot_tloss,'b',label=\"Total Loss\")\n",
    "        plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\n",
    "        plt.figure(2) \n",
    "        aa = plt.plot(plot_loss,'r',label=\"Training\")\n",
    "#        bb = plt.plot(plot_loss2,'g',label=\"Validation\")\n",
    "        \n",
    "        plt.title(\"LOSS\")\n",
    "        plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\n",
    "        plt.figure(3)\n",
    "        cc = plt.plot(plot_acc,'r',label=\"Training\")\n",
    "#        dd = plt.plot(pllot_acc2,'g',label=\"Validation\")\n",
    "        plt.title(\"Accuracy\")\n",
    "        plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\n",
    "        plt.show()\n",
    "#        loss_list2 = []\n",
    "#        val_list = []\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "epoch_start_time = time.time()\n",
    "train_imbatch, train_labatch = sess.run([train_image_batch, train_label_batch])\n",
    "print(train_labatch)\n",
    "print 'Time Elapsed for Epoch:'+ ' is ' + str(\n",
    "            (time.time() - epoch_start_time) / 60.) + ' minutes'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
